{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "# Recurrent Neural Networks \n",
    "We will see text generation and text translation.\n",
    "What is a RNN? We want to include time in our model. We want to model the correlation between samples through time. This is done by means of \"loops\": weights are updated not only on the base of current input, but also using information coming from the past (i.e. state). The way in which the loop is implemented varies between implementation: \n",
    "- RNN\n",
    "- LSTM\n",
    "- GRU\n",
    "Many variants are possible, but there 3 are the basic cells used. <br>\n",
    "\n",
    "We have seen RNN (slide 2): at each time is updating with state coming from the past + input. As we move on, we process our sequence, and we update the weights based on the past information. We can thus with back prop. thorugh time. We can image this loop unrolled: if we do, we can go backward to compute the gradient. The problem of this kind of network is vanishing gradient: weights are multiplied as we go back, and if we want to model a long term dependency it will fail. <br> <br>\n",
    "LSTM are an evalution of this basic cell: it has two kind of components. The first is the cell memory/state, and it is the memory of our network, and then we a set of learnble gates that allow to update content: what to forget from the memory, what to update, and which output we want to produce. GRU merge forget and input gate in a single gate having, thus, less parameters. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RNN  in Keras\n",
    "In keras there is a layer to implement the cells. Units is dimension of the state: how big is encoding of past information. Then, we have parameter typical of the cell: suppose that we process an input sequence of a certain lenght, return_state allow to give as output the output and the state. <br>\n",
    "###### LSTM in Keras\n",
    "In LSTM it is different, since the state is given by the couple [s_h, s_c], where s_c is the state of the memory.\n",
    "Stateful is another important parameter: each time i give a new batch to my cell, if it True, the cell won't discard the previous state, otherwise it will re-init the state every time we give a new input. \n",
    "If we want to model, the temporal correlation between 3 samples, we pick the 3 samples and we give them to the LSTM, and, suppose that we shift to next 3: if stateful is false, the input of the batch is s_h and s_c to 0. If I put stateful to True, we init to state at time 2, instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "Suppose that we have a sentence in input (e.g. \"hello\") and given the previous chars, we want to predict the next one.\n",
    "The samples of our dataset will be the chars, and, we want to do next-char prediction.\n",
    "Our dataset is a book, and we split the dataset in this way: we start at beginning, we keep 3 chars, and then we have target 1, then we go one step ahead, so: <br>\n",
    "hel-l <br>\n",
    "ell-o <br>\n",
    "and so on.<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Data set up\n",
    "full_text = \"Sono un cordero, cioe? asbfoabb asufb uosbfuabuofb aubfuoasbufb auofbusabfu baofsaubf asobubsu.Sono un cordero, cioe? asbfoabb asufb uosbfuabuofb aubfuoasbufb auofbusabfu baofsaubf asobubsu Sono un cordero, cioe? asbfoabb asufb uosbfuabuofb aubfuoasbufb auofbusabfu baofsaubf asobubsu Sono un cordero, cioe? asbfoabb asufb uosbfuabuofb aubfuoasbufb auofbusabfu baofsaubf asobubsuSono un cordero, cioe? asbfoabb asufb uosbfuabuofb aubfuoasbufb auofbusabfu baofsaubf asobubsu Sono un cordero, cioe? asbfoabb asufb uosbfuabuofb aubfuoasbufb auofbusabfu baofsaubf asobubsu Sono un cordero, cioe? asbfoabb asufb uosbfuabuofb aubfuoasbufb auofbusabfu baofsaubf asobubsu Sono un cordero, cioe? asbfoabb asufb uosbfuabuofb aubfuoasbufb auofbusabfu baofsaubf asobubsu Sono un cordero, cioe? asbfoabb asufb uosbfuabuofb aubfuoasbufb auofbusabfu baofsaubf asobubsu Sono un cordero, cioe? asbfoabb asufb uosbfuabuofb aubfuoasbufb auofbusabfu baofsaubf asobubsu \"\n",
    "vocabulary = sorted(list(set(full_text)))\n",
    "\n",
    "ctoi = {c:i for i,c in enumerate(vocabulary)}\n",
    "itoc = {i:c for i,c in enumerate(vocabulary)}\n",
    "\n",
    "seq_lenght = 100 # how many samples we want to consider for the next char prediction\n",
    "\n",
    "# Create dataset\n",
    "full_text_lenght = len(full_text)\n",
    "step = 1\n",
    "X = []\n",
    "Y = []\n",
    "for i in range(0, full_text_lenght - (seq_lenght), step):\n",
    "    sequence = full_text[i: i+seq_lenght]\n",
    "    target = full_text[i+seq_lenght]\n",
    "    X.append([ctoi[c] for c in sequence])\n",
    "    Y.append(ctoi[target])\n",
    "\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "\n",
    "indices = np.arange(len(X)) # we want to shuffle them, to use it for both input and target\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "X = X[indices]\n",
    "Y = Y[indices]\n",
    "\n",
    "num_train = int(0.9 * len(X))\n",
    "x_train = np.array(X[:num_train])\n",
    "y_train = np.array(Y[:num_train])\n",
    "x_valid = np.array(X[num_train:])\n",
    "y_valid = np.array(Y[num_train:])\n",
    "\n",
    "def char_encode(x_, y_):\n",
    "    return tf.one_hot(x_, len(vocabulary)), tf.one_hot(y_, len(vocabulary))\n",
    "\n",
    "bs = 256\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=x_train.shape[0])\n",
    "train_dataset = train_dataset.map(char_encode)\n",
    "train_dataset = train_dataset.batch(bs)\n",
    "train_dataset = train_dataset.repeat()\n",
    "\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((x_valid, y_valid))\n",
    "valid_dataset = valid_dataset.shuffle(buffer_size=x_valid.shape[0])\n",
    "valid_dataset = valid_dataset.map(char_encode)\n",
    "valid_dataset = valid_dataset.batch(bs)\n",
    "valid_dataset = valid_dataset.repeat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to avoid coming from interegers: we want to consider each char as equal: there is no ordinal distance. Each char will be a vector equal to len(vocabulary) and i will have in correspondence to the integer of my char.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_size = 128 # hidden size \n",
    "# Each time i go from time t to time t+1, i have a tensor of dimension h_size. \n",
    "# And this will be also the dimension of the output\n",
    "# If I'm using an LSTM, it is also the dimension of the cell state\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.LSTM(units=h_size, \n",
    "                               return_sequences=True,\n",
    "                               stateful=False,\n",
    "                               batch_input_shape=[None, seq_lenght, len(vocabulary)]))\n",
    "model.add(tf.keras.layers.Dropout(0.4))\n",
    "model.add(tf.keras.layers.LSTM(units=h_size, return_sequences=False, stateful=False))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(tf.keras.layers.Dense(activation=\"softmax\", units=len(vocabulary)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways of doing this stuffs:\n",
    "- Create an LSTM and give one element at time: give x(t), produce h,c and o, store it into a variable, and when it comes t+1, take them and put them in the cell again. I can do this with a for loop\n",
    "- The other way is: give all the inputs and the output is directly the final one, if I set return_seq to False. Otherwise, i will have a vector with an output for each of the time step\n",
    "\n",
    "Other comments:\n",
    "- Stateful=False: we have shuffled the dataset. In the batch size, i have my cicle (for batch in Dataset:) and i want to optimize my network. If i preserve the state in some way I'm connecting the state after the previous word, with the state of the next word, which are not related. Each time, I want to reset. \n",
    "- You can stack LSTM: like in FFNN, we can have multiple layers, and this is such also in this scenario. The output of the first LSTM, can be the input of another layer of LSTM. Don't mess this with unrollmenet. This allows to implement an encoder. At each level of LSTM I can exploit some different feature/abstraction level. We could set a different size for the new LSTM: you may want to compress to have a bottleneck, like in U-net. However, since we want the output of first LSTM, we need to set return_seq to True in the first layer. IN this way we have input of h e l l and so on, that goes in the input of the second LSTM. \n",
    "- At the end, my output is a tensor of dimension batch_size*units. Units is dimension of my state, which is dimension of my output. I'm not directly doing a prediction on the value of the char. My output so far is an encoding. It is the information of what is in my input-> we need a last step of course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 100, 128)          74752     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100, 128)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 17)                2193      \n",
      "=================================================================\n",
      "Total params: 208,529\n",
      "Trainable params: 208,529\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "opt = tf.keras.optimizers.Adam()\n",
    "metrics = [\"accuracy\"]\n",
    "model.compile(loss=loss, optimizer=opt, metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_len = 100\n",
    "start_idx = np.random.randint(0, full_text_lenght - seq_lenght)\n",
    "seed_sentence = full_text[start_idx:start_idx+seq_lenght]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rdero, cioe? asbfoabb asufb uosbfuabuofb aubfuoasbufb auofbusabfu baofsaubf asobubsu Sono un cordero'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I give this sentence of 100 chars, and I want to generate another 100 chars. How to do it? By shifting. I have my seed sequence, and i predict the next, then, i shift, filling with the made prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_onehot = np.zeros([1, seq_lenght, len(vocabulary)])\n",
    "for t_idx, c in enumerate(seed_sequence):\n",
    "    in_onehot[:, t_idx, ctoi[c]] = 1\n",
    "    \n",
    "generated_sentence = seed_sentence\n",
    "\n",
    "for i in range(generation_len):\n",
    "    preds = model.predict(in_onehot)[0]\n",
    "    next_char = np.argmax(preds, -1)\n",
    "    next_char_onehot = np.zeros([1,1,len(vocabulary)])\n",
    "    next_char_onehot[:, :, next_char] = 1\n",
    "    in_oneout = np.concatenate([in_onehot, next_char_onehot], axis=1)\n",
    "    in_onehot = in_oneout[:, 1:, :]\n",
    "    generated_sequence += itoc[next_char]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid to get stuck in the same generation: <br>\n",
    "- In the naive idea, i have the same probability always, choosing the best one.\n",
    "- Sometimes you are allowed to keep not the best, but the ones with a probaiblity over a threshold, and then i random samples in the remaining ones. You can do this with \"temperature\".  The higher the temperature, the higher the probability of having the network to generate no-senses: i am allowing to sample from almost all the vector. The lower, the most confident is the generation. It is an hyper-parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation\n",
    "Given a sentence, i want to encode it into an abstract information, and I want to use it to output the translation.\n",
    "This can in done with S2S model, with an encoder and decoder.                        \n",
    "I have a path that compress the information, and, I use the encoded information to decide and predict the translations.                                                                                     \n",
    "It is very similar to what we have done with chars, but we have to use some kind of embeddings, since we want to capture the semantics in our sentence: one hot encoding does not allow this.                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
